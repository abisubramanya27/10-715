\documentclass{article}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[shortlabels]{enumitem}
\usepackage{bm}
\usepackage{graphicx}

% Declare Operators
\newcommand{\weight}{w}
\newcommand{\bias}{b}
\newcommand{\slack}{\xi}
\newcommand{\dual}{v}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\const}{C}
\newcommand{\margin}{M}
\newcommand{\kernel}{K}
\newcommand{\kernelmap}{\phi}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\param}{\gamma}
\newcommand{\st}{\mathop{\mathrm{subject\,\,to}}}

\usepackage[utf8]{inputenc}

\title{10-715 Fall 2020 Homeworks}

\begin{document}

\begin{center}
{\Large CMU 10-715: Homework 7}\\
Decision Trees \& Unsupervised Learning \\
{\bf DUE: Nov. 21, 2022, 11:59 PM}.\\
\end{center}


\textbf{\large Instructions}:
\begin{itemize}
    \item \textbf{Collaboration policy:} Collaboration on solving the homework is allowed, after you have thought about the problems on your own. It is also OK to get clarification (but not solutions) from books, again after you have thought about the problems on your own. Please don’t search for answers on the web, previous years’ homeworks, etc. (please ask the TAs if you are not sure if you can use a particular reference). There are two requirements: first, cite your collaborators fully and completely (e.g., ``Alice explained to me what is asked in Question 4.3''). Second, write your solution \emph{independently}: close the book and all of your notes, and send collaborators out of the room, so that the solution comes from you only. 
    \item \textbf{Submitting your work:} Assignments should be submitted as PDFs using Gradescope unless explicitly stated otherwise. Each derivation/proof should be completed on a separate page. Submissions can be handwritten, but should be labeled and clearly legible. Else, submission can be written in LaTeX.
    
    \item \textbf{Late days:} For each homework you get three late days to be used only when anything urgent comes up. No points will be deducted for using these late days. We will consider an honor system where we will rely on you to use the late days appropriately.
    

\end{itemize}

\newpage
\section{ [50 pts] Decision Trees}

In the first part of this homework you will implement a decision tree and train it to detect recurrence of breast cancer on the UCI breast cancer dataset. In particular, you need to implement a variant of the ID3 algorithm learned in class.

\subsection{Modified ID3 algorithm}
The ID3 algorithm learned in class only allows for binary target and binary features. The modified version presented can use multivariate features and can create more than two children at any node. \\

\begin{algorithm}[H]
\SetAlgoLined
\textbf{input:} training set S, features A, max depth k, node depth d, \texttt{Gain} measure\\
Create a Root node for the tree \\
\uIf{all examples in S are labeled 1}{\textbf{return} a leaf 1}
\uElseIf{all examples in S are labeled 0}{\textbf{return} a leaf 0}
\uElseIf{$k=d$}{\textbf{return} a leaf whose value is majority of labels in S}
\Else{
$j \leftarrow arg max_{i \in A} \text{\texttt{Gain}}(S,i)$}
Create a subtree with root node having attribute $j$ \\
\For{each unique value $v_i$ of feature $j$}{
    Add a new branch corresponding to $j = v_i$ \\
    \uIf{$|S_{v_{i}}|=0$}{Add a leaf whose value is majority of labels in S below the branch}
    \Else{Add the subtree returned by $ID3(S_{v_i}, A \setminus \{j\}, k, d+1, \text{\texttt{Gain}})$}
    }
Return Root
 \caption{Modified ID3 algorithm}
\end{algorithm}

\subsection{Instructions}
\begin{itemize}
\item The data is stored under the Q1\_data directory. For your reference the dataset description is in breast-cancer.names.txt. We have provided some code for you to start, feel free to modify it.
\item Implement the modified ID3 algorithm stated above. 
\item Use the Gini index metric as the Gain metric.
\item The target variable is the presence of recurrence-events (1: recurrence-events, 0: no-recurrence-events)
\item \textbf{Include all your code at the end of the pdf.}
\end{itemize}

\subsection{Results}
\begin{enumerate}[a]
\item (25 points) Perform k-fold cross-validation with $k=3$. Try with max depth 1, 3, 5, 7 and 9. Plot the training and validation set accuracy for each value of max depth.
\item (15 points) Train the model again with ALL the training samples on the best max depth based on the previous question. Report train and test accuracy.
\item (10 points) What can you say about the relation between train accuracy and max depth? What about validation accuracy and max depth?


\end{enumerate}

% \newpage
\section{ [50 pts] Linkage Based Clustering}

In the second part of this homework you will implement a Linkage Based Clustering algorithm, then try variants of the clustering algorithm by changing the distances of the original space and the distances defined for the clusters. Your main objective will be to apply the unsupervised algorithm to see if the \textit{unsupervised} labels produced by your implementation match the \textit{true} labels of the dataset provided for the homework.\\

\textbf{Since this is an unsupervised algorithm the evaluation will be solely on the report of your findings and not on the performance of the algorithm. Please do not use the \textit{true} labels that are provided to you until the end, once you feel confident that your algorithm has achieved reasonable results}. \\

We first describe single-linkage algorithm. Let the training matrix $\mathbf{X}\in\mathbb{R}^{n\times d}$. For easier exposition we will denote as $D: \mathcal{C}\times\mathcal{C}\mapsto \mathbb{R}$ the distance function between a pair of clusters, and $d:\mathcal{X}\times \mathcal{X}\mapsto \mathbb{R}$ the distance function between observations of the original space.

\vspace{5mm}

\begin{algorithm}[H]
\SetAlgoLined
\textbf{Input:} Training matrix $\mathbf{X}\in\mathbb{R}^{n\times d}$, cluster distance $D$ and number of clusters $k$. * \\
Begin with disjoint clustering (each point considered a cluster). \\
\vspace{2mm}
While number of clusters larger than $k$:
\begin{enumerate}
\item Find the pair of clusters $\mathcal{C}_{i^*}, \mathcal{C}_{j^*}$ with the minimum distance:\\
        $\qquad \mathcal{C}_{i^*}, \mathcal{C}_{j^*} =\underset{\mathcal{C}_{i}, \mathcal{C}_{j} \in \mathcal{C}}{\text{argmin}}
        \left(D(\mathcal{C}_{i},\mathcal{C}_{j})\right)$
\item Merge the clusters into $\mathcal{C}_{new}=\mathcal{C}_{i^*}\cup \mathcal{C}_{j^*}$.
\end{enumerate}
\textbf{Output:} Cluster assignation \\
*Note: the cluster distance $D$ incorporates the selection of $d$.
\caption{Linkage Based Algorithm}
\end{algorithm}

\subsection{Instructions}
\begin{itemize}

\item Read the train matrix available at Q2\_data/train.csv. (Use the labels.csv file zipped up separately inside of Q2\_data for the true labels at the end). Your goal is to perform clustering on this dataset.

\item Code up the single-linkage clustering algorithm.

\item Your goal is to design and choose some distance functions (more details below) so that based on the \emph{training set} you think you have a good clustering algorithm for this data. You can feel free to do whatever you want with the training data, e.g., plot it in any way you want or eyeball the individual datapoints etc. In this process, get intuition behind the data and then either choose from below or design your own distance functions.

\item Once you choose/design your distance function, you set that in stone and then look at the test data. You will be asked to report the performance of the clustering algorithm with your chosen distance function on the test data. \emph{Please keep in mind that our overall bar for a good grade in this homework is \textbf{very low}, so please abide by a honor code of not looking at the test data, and don't worry too much if you don't get a great performance on the test data. The goal is to get your hands dirty and have a good learning experience.} 

\item You will need to use some distance function. Here are some examples of popular distance functions. You are welcome to use one of these distances or any other distance function you are familiar with:
\begin{enumerate}
    \item Euclidean Distance $\qquad d(\mathbf{x}_{i}, \mathbf{x}_{j}) = \sqrt{(\mathbf{x}_{i}-\mathbf{x}_{j})^{\intercal}(\mathbf{x}_{i}-\mathbf{x}_{j})}$
    \item Cosine Distance $\qquad d(\mathbf{x}_{i}, \mathbf{x}_{j}) = 1-\frac{\mathbf{x}_{i}^{\intercal}x_{j}}{||\mathbf{x}_{i}||\mathbf{x}_{j}||}$
    \item City Block Distance $\qquad d(\mathbf{x}_{i}, \mathbf{x}_{j})=\sum^{d}_{h=1}|x_{i,h}-x_{j,h}|$
\end{enumerate}

\item Select among the following variants of the Linkage based algorithm by changing the clusters distance $D: \mathcal{C}\times\mathcal{C}\mapsto \mathbb{R}$ accordingly. You can feel free to pick any one and stick with it or try all three and pick the one which you feel works best -- your choice. 
    \begin{enumerate}
        \item Single linkage
        $\qquad D(\mathcal{C}_{h}, \mathcal{C}_{k})=\underset{\mathbf{x}_{i}\in \mathcal{C}_{h}, \mathbf{x}_{j}\in \mathcal{C}_{k}}{\text{min}}
            \left(d(\mathbf{x}_{i}, \mathbf{x}_{j})\right)$
        \item Max linkage
        $\qquad D(\mathcal{C}_{h}, \mathcal{C}_{k})=\underset{\mathbf{x}_{i}\in \mathcal{C}_{h}, \mathbf{x}_{j}\in \mathcal{C}_{k}}{\text{max}}
            \left(d(\mathbf{x}_{i}, \mathbf{x}_{j})\right)$
        \item Average linkage
        $\qquad D(\mathcal{C}_{h}, \mathcal{C}_{k})= \sum_{\mathbf{x}_i\in\mathcal{C}_{h}}\sum_{\mathbf{x}_j\in\mathcal{C}_{k}} \frac{d(\mathbf{x}_{i}, \mathbf{x}_{j})}{|\mathcal{C}_{h}|*|\mathcal{C}_{k}|}$
    \end{enumerate}
\item We recommend you to try dimensionality reduction techniques as a preprocessing step and an exploratory data analysis tool. Explore the \textbf{train} data to make this decision and feel free to play with multiple functions before deciding the final one! Again, it is important to note that for this part you are not allowed to look at the labels set.

\item \textbf{Include your final code at the end of the pdf.}

\end{itemize}

\subsection{Results}
\begin{enumerate}[a]
\item (25 points) Report the distances that you used and the intuition behind your selection. This includes your selection for the distance between points and the distance for the clusters. Briefly describe the process that you went through for the selection of the distances.

\item (25 points) Create a scatter plot with the cluster labels that you obtained and compare it with a scatter plot with the true labels. For this scatter plot use Principal Components Analysis and plot the first two components.

\end{enumerate}

\end{document}